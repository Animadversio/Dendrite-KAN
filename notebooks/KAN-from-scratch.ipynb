{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 10])\n",
      "torch.Size([15, 50])\n",
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "import math \n",
    "\n",
    "def radial_basis_function(x, grid_pnts, widths):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        x (_type_): _description_\n",
    "        grid_pnts (_type_): _description_\n",
    "        widths (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Calculate the squared Euclidean distance between x and each center\n",
    "    activations = torch.exp( - ((x - grid_pnts) / widths) ** 2)\n",
    "    return activations\n",
    "\n",
    "\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, width=None,\n",
    "                 grid_min=-2.0, grid_max=2.0, grid_size=10, \n",
    "                 trainable_center=False,\n",
    "                 trainable_width=False):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        grid = torch.linspace(grid_min, grid_max, grid_size)\n",
    "        self.centers = nn.Parameter(grid, requires_grad=trainable_center)\n",
    "        if width is None:\n",
    "            width = (grid_max - grid_min) / (grid_size - 1)\n",
    "        self.width = nn.Parameter(torch.tensor(width, dtype=torch.float32), requires_grad=trainable_width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = torch.exp( - ((x[..., None] - self.centers) / self.width)**2)\n",
    "        return activations\n",
    "\n",
    "\n",
    "class RBFKANLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim : int, \n",
    "                 output_dim : int,\n",
    "                 residual=True,\n",
    "                 grid_min=-2.0, grid_max=2.0, grid_size=10, width=None,\n",
    "                 trainable_center=False,\n",
    "                 trainable_width=False):\n",
    "        super(RBFKANLayer, self).__init__()\n",
    "        self.rbf = RBFLayer(width, grid_min, grid_max, grid_size, trainable_center, trainable_width)\n",
    "        basis_coef = torch.randn(input_dim, grid_size) / math.sqrt(input_dim)\n",
    "        self.basis_coef = nn.Parameter(basis_coef, requires_grad=True)\n",
    "        self.residual = residual\n",
    "        if residual:\n",
    "            self.res_act = nn.SiLU()\n",
    "        self.readout = nn.Linear(input_dim, output_dim)\n",
    "        # self.basis_combined = nn.Linear(input_dim * grid_size, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        rbf_act = self.rbf(x) # [batch_size, input_dim, grid_size]\n",
    "        rbf_act = einops.einsum(rbf_act, self.basis_coef, 'B inp grid, inp grid -> B inp')\n",
    "        if self.residual:\n",
    "            rbf_act = rbf_act + self.res_act(x)\n",
    "        out = self.readout(rbf_act)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class RBFKANnet(nn.Module):\n",
    "    def __init__(self, layer_dim_list, residual=True, KAN_config=None):\n",
    "        super(RBFKANnet, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        if KAN_config is None:\n",
    "            KAN_config = {'grid_min': -2.0, 'grid_max': 2.0, 'grid_size': 10, 'width': None,\n",
    "                          'trainable_center': False, 'trainable_width': False}\n",
    "        for i in range(len(layer_dim_list) - 1):\n",
    "            self.layers.append(RBFKANLayer(layer_dim_list[i], layer_dim_list[i+1], \n",
    "                                           residual=residual, **KAN_config))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "rbf_layer = RBFLayer()\n",
    "input_data = torch.randn(10, 2)\n",
    "output = rbf_layer(input_data)\n",
    "print(output.shape)\n",
    "layer = RBFKANLayer(500, 50)\n",
    "data = torch.randn(15, 500)\n",
    "output = layer(data)\n",
    "print(output.shape)\n",
    "net = RBFKANnet([500, 50, 10])\n",
    "output = net(data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RBFKANnet([1, 1], KAN_config={'grid_min': -3.0, 'grid_max': 3.0, 'grid_size': 20, 'width': None,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in RBFKANLayer: 30050\n",
      "Trainable parameters in RBFKANnet: 31060\n",
      "Trainable parameters in MLP: 32714\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "MLP = nn.Sequential(\n",
    "    nn.Linear(500, 64),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "print(\"Trainable parameters in RBFKANLayer:\", count_parameters(layer))\n",
    "print(\"Trainable parameters in RBFKANnet:\", count_parameters(net))\n",
    "print (\"Trainable parameters in MLP:\", count_parameters(MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Lambda(lambda x: x.view(-1)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='~/Datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='~/Datasets', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.309544\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 8960/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.355956\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 9243/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.275057\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9375/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.200089\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9447/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.214981\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9521/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.147768\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9563/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.134384\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9603/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.113770\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9642/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.099980\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9663/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.082237\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9658/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Lambda(lambda x: x.view(-1)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='~/Datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='~/Datasets', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RBFKANnet([28 * 28, 50, 10]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.005)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, test_dataset, train_batch, eval_batch, device, num_epochs=10, lr=0.005, testset_names=None):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=eval_batch, shuffle=False)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    stats = {}\n",
    "    epoch = -1\n",
    "    stats[epoch] = {}\n",
    "    if isinstance(test_dataset, list):\n",
    "        for dataset, label in zip(test_dataset, testset_names):\n",
    "            print(f\"Evaluating on {label}\", end=\"\\t\")\n",
    "            eval_stats = eval_on_dataset(model, dataset, eval_batch, device)\n",
    "            # add prefix to keys\n",
    "            eval_stats = {f\"{label}_{k}\": v for k, v in eval_stats.items()}\n",
    "            stats[epoch].update(eval_stats)\n",
    "    else:\n",
    "        eval_stats = eval_on_dataset(model, test_dataset, eval_batch, device)\n",
    "        stats[epoch].update(eval_stats)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        stats[epoch] = {}\n",
    "        if isinstance(test_dataset, list):\n",
    "            for dataset, label in zip(test_dataset, testset_names):\n",
    "                print(f\"Evaluating on {label}\", end=\"\\t\")\n",
    "                eval_stats = eval_on_dataset(model, dataset, eval_batch, device)\n",
    "                # add prefix to keys\n",
    "                eval_stats = {f\"{label}_{k}\": v for k, v in eval_stats.items()}\n",
    "                stats[epoch].update(eval_stats)\n",
    "        else:\n",
    "            eval_stats = eval_on_dataset(model, test_dataset, eval_batch, device)\n",
    "            stats[epoch].update(eval_stats)\n",
    "        \n",
    "    return model, stats\n",
    "\n",
    "def eval_on_dataset(model, dataset, batch_size, device):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(dataset)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(dataset)} '\n",
    "            f'({100. * correct / len(dataset):.0f}%)')\n",
    "    acc = correct / len(dataset)\n",
    "    return {\"loss\": test_loss, \"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.066652\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.060082\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.051811\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9691/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.041368\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9688/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.039568\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.032715\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9690/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.033347\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9700/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.028179\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9692/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.027380\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9695/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.019722\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9694/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RBFKANnet(\n",
       "  (layers): ModuleList(\n",
       "    (0): RBFKANLayer(\n",
       "      (rbf): RBFLayer()\n",
       "      (res_act): SiLU()\n",
       "      (readout): Linear(in_features=784, out_features=50, bias=True)\n",
       "    )\n",
       "    (1): RBFKANLayer(\n",
       "      (rbf): RBFLayer()\n",
       "      (res_act): SiLU()\n",
       "      (readout): Linear(in_features=50, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RBFKANnet([28 * 28, 50, 10]).to(device)\n",
    "train(model, train_dataset, test_dataset, train_batch=2048, eval_batch=5000, device=\"cuda\", num_epochs=10, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.322063\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 8777/10000 (88%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.395896\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.360773\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9130/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.328451\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9210/10000 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.306554\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9303/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.245835\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9327/10000 (93%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.212305\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9347/10000 (93%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.231029\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9448/10000 (94%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.175529\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9449/10000 (94%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.144383\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9493/10000 (95%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.197676\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9508/10000 (95%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.145599\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9555/10000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.140434\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9553/10000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.138588\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9570/10000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.121138\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9573/10000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.144571\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9531/10000 (95%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.123101\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.127867\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9628/10000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.095717\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9630/10000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.096927\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9626/10000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.108585\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9666/10000 (97%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.087422\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9648/10000 (96%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.093526\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.131294\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9646/10000 (96%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.101269\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9676/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (1): SiLU()\n",
       "  (2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 64),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "train(MLP, train_dataset, test_dataset, train_batch=2048, eval_batch=5000, device=\"cuda\", num_epochs=25, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
